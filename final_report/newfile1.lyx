#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass IIBproject
\begin_preamble
\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\newcommand{\argmax}{\operatornamewithlimits{argmax}}
\newcommand{\xvec}{\boldsymbol{x}}
\newcommand{\thevec}{\boldsymbol{\theta}}
\newcommand{\gvec}{\boldsymbol{\gamma}}
\usepackage{algorithm,algpseudocode}
\end_preamble
\use_default_options true
\begin_modules
eqs-within-sections
\end_modules
\maintain_unincluded_children false
\language british
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize 12
\spacing onehalf
\use_hyperref false
\papersize default
\use_geometry true
\use_amsmath 1
\use_esint 1
\use_mhchem 1
\use_mathdots 0
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2.5cm
\topmargin 2.5cm
\rightmargin 2.5cm
\bottommargin 2.5cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
GPs also scale cubically with the number of observations as they require
 the inversion of a dense covariance matrix.
 This can make it challenging to use them on functions which require a moderatel
y large number of evaluations to optimise.
 
\end_layout

\begin_layout Standard
Bayesian optimisation is used in applications where we require the zeroth
 order optimisation of an expensive to evaluate function 
\begin_inset Formula $f$
\end_inset

.
 Some examples are hyper-parameter tuning in expensive machine learning
 algorithms 
\begin_inset CommandInset citation
LatexCommand cite
key "snoek2012practical"

\end_inset

, experiment design 
\begin_inset CommandInset citation
LatexCommand cite
key "jones1998efficient"

\end_inset

, optimizing control strategies in complex systems 
\begin_inset CommandInset citation
LatexCommand cite
key "cully2015robots"

\end_inset

, and scientific simulation based studies 
\begin_inset CommandInset citation
LatexCommand cite
key "brochu2010tutorial"

\end_inset

.
 Bayesian optimization finds the optimum of 
\emph on

\begin_inset Formula $f$
\end_inset


\emph default
 by using as few queries as possible by managing exploration and exploitation
 
\begin_inset CommandInset citation
LatexCommand cite
key "mockus1978application"

\end_inset

.
 The algorithm relies on querying a distribution over functions defined
 by a relatively cheap surrogate model.
 An accurate model for this distribution over functions is critical to the
 effectiveness of the approach and is typically fit using Gaussian processes
 (GPs) 
\begin_inset CommandInset citation
LatexCommand cite
key "rasmussen2006gaussian"

\end_inset

.
 However, GPs can only model a limited class of functions well.
 Using GPs on a high dimensional input is also very challenging restricting
 the dimensionality of the functions which can be optimised by this method
 
\begin_inset CommandInset citation
LatexCommand cite
key "wang2013bayesian"

\end_inset

.
 GPs also scale cubically with the number of observations as they require
 the inversion of a dense covariance matrix.
 This can make it challenging to use them on functions which require a moderatel
y large number of evaluations to optimise.
 
\end_layout

\begin_layout Standard
Another promising avenue would be to use this method 
\end_layout

\begin_layout Standard
\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Standard
<<justify high dimensional>>Neural networks are known to be adept at dealing
 with high dimensional inputs.
 .
 Multiple transforms can extract relevant variation directions ...
 
\end_layout

\begin_layout Standard
As demonstrated by autoencoders, neural networks can automatically discover
 efficient low dimensional codes from inputs.
 
\end_layout

\begin_layout Standard
A BN
\end_layout

\begin_layout Standard
Such a method, can also potentially use the feature selection and dimensionality
 reduction properties of neural networks to effectively perform Bayesian
 optimisation for high dimensional functions.
 
\end_layout

\end_body
\end_document
