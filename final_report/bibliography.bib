%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/

%% Created for Devang Agrawal at 2016-01-12 18:33:38 +0000 


%% Saved with string encoding Unicode (UTF-8) 



@article{Snoek:2015aa,
	Author = {Jasper Snoek and Oren Rippel and Kevin Swersky and Ryan Kiros and Nadathur Satish and Narayanan Sundaram and Md. Mostofa Ali Patwary and Prabhat and Ryan P. Adams},
	Date-Added = {2016-01-12 18:33:19 +0000},
	Date-Modified = {2016-01-12 18:33:19 +0000},
	Eprint = {1502.05700},
	Month = {02},
	Title = {Scalable Bayesian Optimization Using Deep Neural Networks},
	Url = {http://arxiv.org/abs/1502.05700},
	Year = {2015},
	Bdsk-Url-1 = {http://arxiv.org/abs/1502.05700}}

@article{Kandasamy:2015aa,
	Abstract = {Bayesian Optimisation (BO) is a technique used in optimising a $D$-dimensional function which is typically expensive to evaluate. While there have been many successes for BO in low dimensions, scaling it to high dimensions has been notoriously difficult. Existing literature on the topic are under very restrictive settings. In this paper, we identify two key challenges in this endeavour. We tackle these challenges by assuming an additive structure for the function. This setting is substantially more expressive and contains a richer class of functions than previous work. We prove that, for additive functions the regret has only linear dependence on $D$ even though the function depends on all $D$ dimensions. We also demonstrate several other statistical and computational benefits in our framework. Via synthetic examples, a scientific simulation and a face detection problem we demonstrate that our method outperforms naive BO on additive functions and on several examples where the function is not additive.},
	Author = {Kirthevasan Kandasamy and Jeff Schneider and Barnabas Poczos},
	Date-Added = {2016-01-12 18:24:55 +0000},
	Date-Modified = {2016-01-12 18:24:55 +0000},
	Eprint = {1503.01673},
	Month = {03},
	Title = {High Dimensional Bayesian Optimisation and Bandits via Additive Models},
	Url = {http://arxiv.org/abs/1503.01673},
	Year = {2015},
	Bdsk-Url-1 = {http://arxiv.org/abs/1503.01673}}

@unpublished{Bengio-et-al-2015-Book,
	Author = {Yoshua Bengio and Ian J. Goodfellow and Aaron Courville},
	Date-Added = {2016-01-12 18:24:01 +0000},
	Date-Modified = {2016-01-12 18:24:01 +0000},
	Note = {Book in preparation for MIT Press},
	Title = {Deep Learning},
	Url = {http://www.iro.umontreal.ca/~bengioy/dlbook},
	Year = {2015},
	Bdsk-Url-1 = {http://www.iro.umontreal.ca/~bengioy/dlbook}}

@article{wang2013bayesian,
	Author = {Wang, Ziyu and Zoghi, Masrour and Hutter, Frank and Matheson, David and de Freitas, Nando},
	Date-Added = {2016-01-12 18:22:22 +0000},
	Date-Modified = {2016-01-12 18:22:22 +0000},
	Journal = {arXiv preprint arXiv:1301.1942},
	Title = {Bayesian optimization in a billion dimensions via random embeddings},
	Year = {2013}}


	@article{mockus1978application,
  title={The application of Bayesian methods for seeking the extremum},
  author={Mockus, Jonas and Tiesis, Vytautas and Zilinskas, Antanas},
  journal={Towards Global Optimization},
  volume={2},
  number={117-129},
  pages={2},
  year={1978},
  publisher={Amsterdam: Elsevier}
}

@article{jones1998efficient,
  title={Efficient global optimization of expensive black-box functions},
  author={Jones, Donald R and Schonlau, Matthias and Welch, William J},
  journal={Journal of Global optimization},
  volume={13},
  number={4},
  pages={455--492},
  year={1998},
  publisher={Springer}
}

@article{mackay1995probable,
  title={Probable networks and plausible predictions, a review of practical Bayesian methods for supervised neural networks},
  author={MacKay, David JC},
  journal={Network: Computation in Neural Systems},
  volume={6},
  number={3},
  pages={469--505},
  year={1995},
  publisher={Taylor \& Francis}
}




@inproceedings{snoek2012practical,
  title={Practical Bayesian optimization of machine learning algorithms},
  author={Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P},
  booktitle={Advances in neural information processing systems},
  pages={2951--2959},
  year={2012}
}

@inproceedings{lizotte2007automatic,
  title={Automatic Gait Optimization with Gaussian Process Regression.},
  author={Lizotte, Daniel J and Wang, Tao and Bowling, Michael H and Schuurmans, Dale},
  booktitle={IJCAI},
  volume={7},
  pages={944--949},
  year={2007}
}

@article{denil2012learning,
  title={Learning where to attend with deep architectures for image tracking},
  author={Denil, Misha and Bazzani, Loris and Larochelle, Hugo and de Freitas, Nando},
  journal={Neural computation},
  volume={24},
  number={8},
  pages={2151--2184},
  year={2012},
  publisher={MIT Press}
}

@article{mackay1992practical,
  title={A practical Bayesian framework for backpropagation networks},
  author={MacKay, David JC},
  journal={Neural computation},
  volume={4},
  number={3},
  pages={448--472},
  year={1992},
  publisher={MIT Press}
}

@inproceedings{bergstra2010theano,
  title={Theano: A CPU and GPU math compiler in Python},
  author={Bergstra, James and Breuleux, Olivier and Bastien, Fr{\'e}d{\'e}ric and Lamblin, Pascal and Pascanu, Razvan and Desjardins, Guillaume and Turian, Joseph and Warde-Farley, David and Bengio, Yoshua},
  booktitle={Proc. 9th Python in Science Conf},
  pages={1--7},
  year={2010}
}

@MISC{Bastien-Theano-2012,
        author = {Bastien, Fr{\'{e}}d{\'{e}}ric and Lamblin, Pascal and Pascanu, Razvan and Bergstra, James and Goodfellow, Ian J. and Bergeron, Arnaud and Bouchard, Nicolas and Bengio, Yoshua},
         title = {Theano: new features and speed improvements},
          year = {2012},
  howpublished = {Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop},
      abstract = {Theano is a linear algebra compiler that optimizes a user’s symbolically-speciﬁed
mathematical computations to produce efﬁcient low-level implementations. In
this paper, we present new features and efﬁciency improvements to Theano, and
benchmarks demonstrating Theano’s performance relative to Torch7, a recently
introduced machine learning library, and to RNNLM, a C++ library targeted at
recurrent neural networks.}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}
@article{zeiler2012adadelta,
  title={ADADELTA: An adaptive learning rate method},
  author={Zeiler, Matthew D},
  journal={arXiv preprint arXiv:1212.5701},
  year={2012}
}

@article{hinton2006reducing,
  title={Reducing the dimensionality of data with neural networks},
  author={Hinton, Geoffrey E and Salakhutdinov, Ruslan R},
  journal={Science},
  volume={313},
  number={5786},
  pages={504--507},
  year={2006},
  publisher={American Association for the Advancement of Science}
}

@inproceedings{hinton2008using,
  title={Using deep belief nets to learn covariance kernels for Gaussian processes},
  author={Hinton, Geoffrey E and Salakhutdinov, Ruslan R},
  booktitle={Advances in neural information processing systems},
  pages={1249--1256},
  year={2008}
}

@book{neal1996bayesian,
  title={Bayesian learning for neural networks},
  author={Neal, Radford M},
  volume={118},
  year={1996},
  publisher={Springer Science \& Business Media}
}
@article{dugan2011healthy,
  title={Healthy computer use for computer science},
  author={Dugan Jr, Robert F and Dugan, Rebecca M and Trabucco, Corinna},
  journal={Journal of Computing Sciences in Colleges},
  volume={27},
  number={1},
  pages={9--15},
  year={2011},
  publisher={Consortium for Computing Sciences in Colleges}
}

@article{andrieu2003introduction,
  title={An introduction to MCMC for machine learning},
  author={Andrieu, Christophe and De Freitas, Nando and Doucet, Arnaud and Jordan, Michael I},
  journal={Machine learning},
  volume={50},
  number={1-2},
  pages={5--43},
  year={2003},
  publisher={Springer}
}

@article{metropolis1953equation,
  title={Equation of state calculations by fast computing machines},
  author={Metropolis, Nicholas and Rosenbluth, Arianna W and Rosenbluth, Marshall N and Teller, Augusta H and Teller, Edward},
  journal={The journal of chemical physics},
  volume={21},
  number={6},
  pages={1087--1092},
  year={1953},
  publisher={AIP Publishing}
}

@article{DUANE1987216,
title = "Hybrid Monte Carlo",
journal = "Physics Letters B",
volume = "195",
number = "2",
pages = "216 - 222",
year = "1987",
note = "",
issn = "0370-2693",
doi = "http://dx.doi.org/10.1016/0370-2693(87)91197-X",
url = "http://www.sciencedirect.com/science/article/pii/037026938791197X",
author = "Simon Duane and A.D. Kennedy and Brian J. Pendleton and Duncan Roweth",

}

@book{bishop2006pattern,
  title={Pattern recognition and machine learning},
  author={Bishop, Christopher M},
  volume={1},
  year={2006},
  publisher={springer}
}

@article{neal1993probabilistic,
  title={Probabilistic inference using Markov chain Monte Carlo methods},
  author={Neal, Radford M},
  year={1993},
  publisher={Department of Computer Science, University of Toronto}
}

@MISC{Ranzato10factored3-way,
    author = {Marc' Aurelio Ranzato and Alex Krizhevsky and Geoffrey E. Hinton},
    title = {Factored 3-Way Restricted Boltzmann Machines For Modeling Natural Images},
    year = {2010}
}

@article{brochu2010tutorial,
  title={A tutorial on {B}ayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning},
  author={Brochu, Eric and Cora, Vlad M and De Freitas, Nando},
  journal={arXiv preprint arXiv:1012.2599},
  year={2010}
}

@book{forrester2008engineering,
  title={Engineering design via surrogate modelling: a practical guide},
  author={Forrester, Alexander and Sobester, Andras and Keane, Andy},
  year={2008},
  publisher={John Wiley \& Sons}
}

@article{picheny2013benchmark,
  title={A benchmark of kriging-based infill criteria for noisy optimization},
  author={Picheny, Victor and Wagner, Tobias and Ginsbourger, David},
  journal={Structural and Multidisciplinary Optimization},
  volume={48},
  number={3},
  pages={607--626},
  year={2013},
  publisher={Springer}
}

@article{adorio2005mvf,
  title={Mvf-multivariate test functions library in c for unconstrained global optimization},
  author={Adorio, Ernesto P and Diliman, U},
  journal={Quezon City, Metro Manila, Philippines},
  year={2005}
}

@article{molga2005test,
  title={Test functions for optimization needs},
  author={Molga, Marcin and Smutnicki, Czes{\l}aw},
  journal={Test functions for optimization needs},
  year={2005}
}

@inproceedings{mhaskar1994choose,
  title={How to choose an activation function},
  author={Mhaskar, Hrushikesh Narhar and Micchelli, Charles A},
  booktitle={Advances in Neural Information Processing Systems},
  pages={319--326},
  year={1994}
}




@article{dixon1978global,
  title={The global optimization problem: an introduction},
  author={Dixon, LCW and Szeg{\"o}, GP},
  journal={Towards global optimization},
  volume={2},
  pages={1--15},
  year={1978},
  publisher={North-Holland Amsterdam}
}

@article{cybenko1989approximation,
  title={Approximation by superpositions of a sigmoidal function},
  author={Cybenko, George},
  journal={Mathematics of control, signals and systems},
  volume={2},
  number={4},
  pages={303--314},
  year={1989},
  publisher={Springer}
}
@article{Hornik1991251,
title = "Approximation capabilities of multilayer feedforward networks ",
journal = "Neural Networks ",
volume = "4",
number = "2",
pages = "251 - 257",
year = "1991",
note = "",
issn = "0893-6080",
doi = "http://dx.doi.org/10.1016/0893-6080(91)90009-T",
url = "http://www.sciencedirect.com/science/article/pii/089360809190009T",
author = "Kurt Hornik",
keywords = "Multilayer feedforward networks",
keywords = "Activation function",
keywords = "Universal approximation capabilities",
keywords = "Input environment measure",
keywords = "Lp(μ) approximation",
keywords = "Uniform approximation",
keywords = "Sobolev spaces",
keywords = "Smooth approximation ",
abstract = "We show that standard multilayer feedforward networks with as few as a single hidden layer and arbitrary bounded and nonconstant activation function are universal approximators with respect to Lp(μ) performance criteria, for arbitrary finite input environment measures μ, provided only that sufficiently many hidden units are available. If the activation function is continuous, bounded and nonconstant, then continuous mappings can be learned uniformly over compact input sets. We also give very general conditions ensuring that networks with sufficiently smooth activation functions are capable of arbitrarily accurate approximation to a function and its derivatives. "
}
@article{funahashi1989approximate,
  title={On the approximate realization of continuous mappings by neural networks},
  author={Funahashi, Ken-Ichi},
  journal={Neural networks},
  volume={2},
  number={3},
  pages={183--192},
  year={1989},
  publisher={Elsevier}
}

@article{rumelhart1988learning,
  title={Learning representations by back-propagating errors},
  author={Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
  journal={Cognitive modeling},
  volume={5},
  number={3},
  pages={1},
  year={1988}
}
@article{stone1974cross,
  title={Cross-validatory choice and assessment of statistical predictions},
  author={Stone, Mervyn},
  journal={Journal of the royal statistical society. Series B (Methodological)},
  pages={111--147},
  year={1974},
  publisher={JSTOR}
}

@inproceedings{hoffman2011portfolio,
  title={Portfolio Allocation for Bayesian Optimization.},
  author={Hoffman, Matthew D and Brochu, Eric and de Freitas, Nando},
  booktitle={UAI},
  pages={327--336},
  year={2011},
  organization={Citeseer}
}

@article{srinivas2009gaussian,
  title={Gaussian process optimization in the bandit setting: No regret and experimental design},
  author={Srinivas, Niranjan and Krause, Andreas and Kakade, Sham M and Seeger, Matthias},
  journal={arXiv preprint arXiv:0912.3995},
  year={2009}
}

@article{bengio2013representation,
  title={Representation learning: A review and new perspectives},
  author={Bengio, Yoshua and Courville, Aaron and Vincent, Pierre},
  journal={Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  volume={35},
  number={8},
  pages={1798--1828},
  year={2013},
  publisher={IEEE}
}
@article{mnih2015human,
  title={Human-level control through deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
  journal={Nature},
  volume={518},
  number={7540},
  pages={529--533},
  year={2015},
  publisher={Nature Publishing Group}
}

@article{silver2016mastering,
  title={Mastering the game of Go with deep neural networks and tree search},
  author={Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and others},
  journal={Nature},
  volume={529},
  number={7587},
  pages={484--489},
  year={2016},
  publisher={Nature Publishing Group}
}



@book{rasmussen2006gaussian,
  title={Gaussian Processes for Machine Learning},
  author={Rasmussen, C. E. and Williams, C. K. I.},
  year={2006},
  publisher={the MIT Press}
}

@inproceedings{marchant2012bayesian,
  title={Bayesian optimisation for intelligent environmental monitoring},
  author={Marchant, Roman and Ramos, Fabio},
  booktitle={Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on},
  pages={2242--2249},
  year={2012},
  organization={IEEE}
}

@article{cully2015robots,
  title={Robots that can adapt like animals},
  author={Cully, Antoine and Clune, Jeff and Tarapore, Danesh and Mouret, Jean-Baptiste},
  journal={Nature},
  volume={521},
  number={7553},
  pages={503--507},
  year={2015},
  publisher={Nature Publishing Group}
}

@article{gonzalez2015bayesian,
  title={Bayesian optimization for synthetic gene design},
  author={Gonzalez, Javier and Longworth, Joseph and James, David C and Lawrence, Neil D},
  journal={arXiv preprint arXiv:1505.01627},
  year={2015}
}

@article{duvenaud2013structure,
  title={Structure discovery in nonparametric regression through compositional kernel search},
  author={Duvenaud, David and Lloyd, James Robert and Grosse, Roger and Tenenbaum, Joshua B and Ghahramani, Zoubin},
  journal={arXiv preprint arXiv:1302.4922},
  year={2013}
}

@inproceedings{salakhutdinov2007restricted,
  title={Restricted Boltzmann machines for collaborative filtering},
  author={Salakhutdinov, Ruslan and Mnih, Andriy and Hinton, Geoffrey},
  booktitle={Proceedings of the 24th international conference on Machine learning},
  pages={791--798},
  year={2007},
  organization={ACM}
}
@incollection{mackay1998introduction,
  title={Introduction to monte carlo methods},
  author={MacKay, David JC},
  booktitle={Learning in graphical models},
  pages={175--204},
  year={1998},
  publisher={Springer}
}

@phdthesis{duvenaud2014automatic,
  title={Automatic model construction with Gaussian processes},
  author={Duvenaud, David},
  year={2014},
  school={University of Cambridge}
}

@article{geman1984stochastic,
  title={Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images},
  author={Geman, Stuart and Geman, Donald},
  journal={Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  number={6},
  pages={721--741},
  year={1984},
  publisher={IEEE}
}
@article{casella1992explaining,
  title={Explaining the Gibbs sampler},
  author={Casella, George and George, Edward I},
  journal={The American Statistician},
  volume={46},
  number={3},
  pages={167--174},
  year={1992},
  publisher={Taylor \& Francis}
}
@article{lloyd2014automatic,
  title={Automatic construction and natural-language description of nonparametric regression models},
  author={Lloyd, James Robert and Duvenaud, David and Grosse, Roger and Tenenbaum, Joshua B and Ghahramani, Zoubin},
  journal={arXiv preprint arXiv:1402.4304},
  year={2014}
}
