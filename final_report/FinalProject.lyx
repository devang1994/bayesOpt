#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass IIBproject
\use_default_options true
\begin_modules
eqs-within-sections
\end_modules
\maintain_unincluded_children false
\language british
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize 12
\spacing onehalf
\use_hyperref false
\papersize default
\use_geometry true
\use_amsmath 1
\use_esint 1
\use_mhchem 1
\use_mathdots 0
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2.5cm
\topmargin 2.5cm
\rightmargin 2.5cm
\bottommargin 2.5cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Bayesian Optimisation using Neural Networks
\end_layout

\begin_layout Author
Devang Agrawal (Queens')
\end_layout

\begin_layout Project Group
F-
\end_layout

\begin_layout Summary
This project explored the use of neural networks for Bayesian Optimisation.
 No more than 100 words.
 
\end_layout

\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
Bayesian Optimisation is a very interesting problem.
 It has been used in ..
 .
 Neural networks can be used for dimensionality reduction and can be used
 for feature selection.
 It is not trivial how to build probabilistic models using Neural Networks.
 Several different options are available and two particular ones were explored
 in this report.
 Using a pragmatic approach ....
 .
 Alternatively a full Bayesian Neural network was used ....
 .
 These functions can be very useful for non-stationary functions.
\end_layout

\begin_layout Section
Neural Networks
\begin_inset CommandInset label
LatexCommand label
name "sec:Neural-Networks"

\end_inset


\end_layout

\begin_layout Standard
Neural networks represent multiple non-linear transforms of the input data
\begin_inset CommandInset citation
LatexCommand cite
key "Bengio-et-al-2015-Book"

\end_inset

 .
 Excellent at Feature Selection 
\begin_inset Note Note
status open

\begin_layout Plain Layout
can also use radfords book 
\end_layout

\end_inset


\end_layout

\begin_layout Section
Markov Chain Monte Carlo methods
\end_layout

\begin_layout Standard
Markov chain Monte Carlo(MCMC) methods are a class of algorithms for sampling
 from a desired probability distribution
\begin_inset CommandInset citation
LatexCommand cite
key "andrieu2003introduction,neal1996bayesian"

\end_inset

.
 A Markov chain is constructed with the desired distribution as its equilibrium
 distribution.
 These models are widely applied to Bayesian models in statistics.
 MCMC methods make no assumptions about the about the form of the underlying
 distribution.
 In theory they can take account of multiple modes and the possibility that
 the main contribution to the integral may come from areas not in the vicinity
 of any mode.
 In practise though it can under certain circumstances they can take a very
 long time to converge to the desired distribution.
 This is the main disadvantage of using MCMC methods.
 
\end_layout

\begin_layout Standard
In Bayesian learning we often encounter situations where we need to evaluate
 the expectation of a function 
\begin_inset Formula $f(\theta)$
\end_inset

 with respect to the posterior probability density of the model parameters.
 Writing the posterior as 
\begin_inset Formula $Q(\theta)$
\end_inset

 , the expectation can be expressed as 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
E[f]\;=\;\intop f(\theta)Q(\theta)d\theta
\end{equation}

\end_inset

Such expectations can be estimated by the 
\emph on
Monte Carlo 
\emph default
method, using a sample of values from 
\begin_inset Formula $Q$
\end_inset

: 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
E[f]\;\thickapprox\;\frac{1}{N}\sum_{t=1}^{N}f(\theta^{(t)})\label{eq:carloIntegration}
\end{equation}

\end_inset

where 
\begin_inset Formula $\theta^{(1)},\dots,\theta^{(N)}$
\end_inset

 are generated by a process that results in each of them having the distribution
 defined by 
\begin_inset Formula $Q$
\end_inset

.
 In simple Monte Carlo methods, the 
\begin_inset Formula $\theta^{(t)}$
\end_inset

 are independent, however when 
\begin_inset Formula $Q$
\end_inset

 is a complicated distribution , generating such independent values is often
 infeasible.
 However it is often possible to generate a series of dependent values.
 The Monte Carlo integration formula of equation (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:carloIntegration"

\end_inset

) still gives an unbiased estimate of 
\begin_inset Formula $E[f]$
\end_inset

 even when the 
\begin_inset Formula $\theta^{(t)}$
\end_inset

 are dependent as long as the independence is not too great
\begin_inset CommandInset citation
LatexCommand cite
key "neal1996bayesian"

\end_inset

.
 The estimate will still converge to the true value as N increases.
\end_layout

\begin_layout Standard
Such a series of dependent values can be generated using a 
\emph on
Markov Chain
\emph default
 that has 
\begin_inset Formula $Q$
\end_inset

 as its stationary distribution.
 The chain can be defined by giving an 
\emph on
initial distribution
\emph default
 for the first state of the chain, 
\begin_inset Formula $\theta^{(1)}$
\end_inset

, and a set of 
\emph on
transition probabilities 
\emph default
for a new state, 
\begin_inset Formula $\theta^{(t+1)}$
\end_inset

, to follow the current state, 
\begin_inset Formula $\theta^{(t)}.$
\end_inset

 The probability densities for these transitions can be written as 
\begin_inset Formula $T(\theta^{(t+1)}|\theta^{(t)})$
\end_inset

.
 A 
\emph on
stationary (or invariant)
\emph default
 distribution, 
\begin_inset Formula $Q$
\end_inset

, is one that persists once it is established.
 This means that if 
\begin_inset Formula $\theta^{(t)}$
\end_inset

 has the distribution 
\begin_inset Formula $Q$
\end_inset

, then 
\begin_inset Formula $\theta^{(t')}$
\end_inset

 will have the same distribution for all 
\begin_inset Formula $t'>t$
\end_inset

.
 This invariance condition can be written as follows:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
Q(\theta')=\int T(\theta'|\theta)Q(\theta)d\theta
\end{equation}

\end_inset

The invariance with respect to 
\begin_inset Formula $Q$
\end_inset

 is implied by the stronger 
\emph on
detailed balance
\emph default
 condition -- that for all 
\begin_inset Formula $\theta$
\end_inset

 and 
\begin_inset Formula $\theta'$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
T(\theta'|\theta)Q(\theta)=T(\theta|\theta')Q(\theta')\label{eq:deTAILed balance}
\end{equation}

\end_inset

A chain satisfying detailed balance is said to be 
\emph on
reversible.
\end_layout

\begin_layout Standard
An 
\emph on
ergodic 
\emph default
Markov chain has a unique invariant equilibrium distribution, to which it
 converges from any initial state.
 If we can find a Markov Chain that has 
\begin_inset Formula $Q$
\end_inset

 as its equilibrium distribution, then we can find find expectations with
 respect to 
\begin_inset Formula $Q$
\end_inset

 by using equation (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:carloIntegration"

\end_inset

) .
 In this case 
\begin_inset Formula $\theta^{(1)},\dots,\theta^{(N)}$
\end_inset

 are the states of the chain, some of the early states can be discarded
 since they are not representative of the equilibrium distribution.
 
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
In summary ?
\end_layout

\end_inset

To use MCMC methods to estimate an expectation with respect to some distribution
 
\begin_inset Formula $Q$
\end_inset

, we need to construct an ergodic Markov chain with 
\begin_inset Formula $Q$
\end_inset

 as the equilibrium distribution.
 The chain should rapidly converge to this distribution and the states visited
 once equilibrium is reached should not be highly dependent.
 
\begin_inset Note Note
status open

\begin_layout Plain Layout
Page 25 maybe write more
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
The Metropolis algorithm
\begin_inset CommandInset label
LatexCommand label
name "sub:The-Metropolis-algorithm"

\end_inset


\end_layout

\begin_layout Standard
The Metropolis algorithm was introduced by Metropolis et.al in their classic
 paper in 1953
\begin_inset CommandInset citation
LatexCommand cite
key "metropolis1953equation"

\end_inset

 .
 In the Markov chain defined by the metropolis algorithm, a new state, 
\begin_inset Formula $\theta^{(t+1)}$
\end_inset

, is generated from the previous state, 
\begin_inset Formula $\theta^{(t)}$
\end_inset

, by first generating a 
\emph on
candidate state 
\emph default
using a specified 
\emph on
proposal distribution, 
\emph default
and then deciding whether or not to accept that state based on its probability
 density relative to the old state, with respect to the desired invariant
 distribution, 
\begin_inset Formula $Q.$
\end_inset

 If accepted, the candidate state, becomes the next state of the Markov
 chain; if it is instead rejected, the new state remains the same as the
 old state.
 
\end_layout

\begin_layout Standard
In detail, the transition from 
\begin_inset Formula $\theta^{(t)}$
\end_inset

 to 
\begin_inset Formula $\theta^{(t+1)}$
\end_inset

 is defined as follows:
\end_layout

\begin_layout Enumerate
Generate a candidate state, 
\begin_inset Formula $\theta^{*}$
\end_inset

, from a proposal distribution.
 The proposal distribution may depend on the current state, its density
 is by 
\begin_inset Formula $S(\theta^{*}\,|\,\theta^{(t)})$
\end_inset

.
 It is noted that the proposal distribution must be symmetrical, satisfying
 the condition 
\begin_inset Formula $S(\theta'\,|\,\theta)=S(\theta\,|\,\theta')$
\end_inset

.
\end_layout

\begin_layout Enumerate
If 
\begin_inset Formula $Q(\theta^{*})\geq Q(\theta^{t})$
\end_inset

, accept the candidate state; otherwise accept the candidate state with
 probability 
\begin_inset Formula $Q(\theta')/Q(\theta)$
\end_inset

.
 For 
\begin_inset Formula $\theta'\neq\theta$
\end_inset

, the overall transition probability is then given by:
\begin_inset Formula 
\begin{equation}
T(\theta'|\theta)=S(\theta'\,|\,\theta)min(1,Q(\theta')/Q(\theta))\label{eq:overallTransitionmetropolis}
\end{equation}

\end_inset


\end_layout

\begin_layout Enumerate
If the candidate state is accepted, let 
\begin_inset Formula $\theta^{t+1}=\theta^{*}$
\end_inset

.
 However if it was not accepted, then set 
\begin_inset Formula $\theta^{(t+1)}=\theta^{(t)}$
\end_inset

.
\end_layout

\begin_layout Standard
Following the overall transition probability in equation (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:overallTransitionmetropolis"

\end_inset

), the detailed balance condition (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:deTAILed balance"

\end_inset

) can be verified as follows:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
T(\theta'|\theta)Q(\theta) & =S(\theta'\,|\,\theta)min(1,Q(\theta')/Q(\theta))Q(\theta)\\
 & =S(\theta'\,|\,\theta)min(Q(\theta),Q(\theta'))\\
 & =S(\theta\,|\,\theta')min(Q(\theta'),Q(\theta))\\
 & =S(\theta\,|\,\theta')min(1,Q(\theta)/Q(\theta'))Q(\theta')\\
 & =T(\theta|\theta')Q(\theta')
\end{align*}

\end_inset

Therefore the Metropolis updates leave 
\begin_inset Formula $Q$
\end_inset

 invariant.
 
\end_layout

\begin_layout Standard
Many choices are available for the proposal distribution of the Metropolis
 algorithm.
 One simple and popular choice is a Gaussian distribution centred on 
\begin_inset Formula $\theta^{(t)},$
\end_inset

with the variance chosen so that the probability of the candidate being
 accepted is reasonably high.
 Very low acceptance rates can be bad as they lead to successive samples
 being highly dependent.
 When sampling from a complex, high-dimensional distribution, the standard
 deviation of of the proposal distributions typically has to be very small
 to compared to the extent of 
\begin_inset Formula $Q.$
\end_inset

 This is because large changes will almost certainly lead to regions of
 low probability.
 A large number of steps will be required to move to a distant point in
 the distribution.
 The problem is made worse by the fact that these movements will take the
 form of a random walk, rather than a systematic traversal.
\end_layout

\begin_layout Standard
Simple forms of the Metropolis algorithm can be very slow when applied to
 problems such as Bayesian learning for neural networks
\begin_inset CommandInset citation
LatexCommand cite
key "neal1996bayesian"

\end_inset

.
 As will be seen in section (
\begin_inset CommandInset ref
LatexCommand ref
reference "sub:The-hybrid-Monte"

\end_inset

) , this problem can be alleviated by using the hybrid Monte Carlo algorithm
\begin_inset CommandInset citation
LatexCommand cite
key "DUANE1987216"

\end_inset

, in which candidate states are generated by a dynamical method which can
 avoid the random walk aspect of the exploration.
\end_layout

\begin_layout Subsection
The hybrid Monte Carlo algorithm
\begin_inset CommandInset label
LatexCommand label
name "sub:The-hybrid-Monte"

\end_inset


\end_layout

\begin_layout Standard
The hybrid Monte Carlo algorithm was originally developed by Duane et.al.
 for application in quantum chromodynamics
\begin_inset CommandInset citation
LatexCommand cite
key "DUANE1987216"

\end_inset

.
 Radford Neal in his book on Bayesian Learning for Neural Networks
\begin_inset CommandInset citation
LatexCommand cite
key "neal1996bayesian"

\end_inset

successfully applies this technique to Bayesian learning.
 The algorithm merges the Metropolis algorithm (reviewed in sec 
\begin_inset CommandInset ref
LatexCommand ref
reference "sub:The-Metropolis-algorithm"

\end_inset

) with sampling techniques based on dynamical simulation.
 It generates a sample of points drawn from some specified distribution
 which can then be used to form Monte Carlo estimates for the expectations
 of various functions with respect to this distribution.
 
\begin_inset Note Note
status open

\begin_layout Plain Layout
maybe remove.
 maybe include reference to a relevant equation where this is important
\end_layout

\end_inset

 For Bayesian learning, we often wish to sample from the posterior distribution
 given the training data, and are interested in estimating the expectations
 needed to make predictions for test cases.
\end_layout

\begin_layout Standard
The hybrid Monte Carlo algorithm is expressed in terms of sampling from
 the 
\emph on
canonical 
\emph default
(or 
\emph on
Boltzmann
\emph default
) distribution for the state of a physical system, which is defined in terms
 of an energy function.
 The algorithm can be used to sample from any distribution for a set of
 real-valued variables for which the derivatives of the probability density
 can be computed.
 For describing the formulation of this algorithm it is convenient to retain
 the physical terminology even in non-physical contexts.
 The problem can then be described in terms of an energy function for a
 fictitious physical system.
 
\end_layout

\begin_layout Standard
Accordingly, suppose we wish to sample from some distribution for a 
\begin_inset Quotes eld
\end_inset

position
\begin_inset Quotes erd
\end_inset

 variable, 
\begin_inset Formula $\mathbf{q}$
\end_inset

, which has 
\begin_inset Formula $n$
\end_inset

 real-valued components, 
\begin_inset Formula $q_{i}$
\end_inset

.
 When used for Bayesian neural networks in section (
\begin_inset CommandInset ref
LatexCommand ref
reference "sub:Using-a-full_BNN"

\end_inset

) , 
\begin_inset Formula $\mathbf{q}$
\end_inset

 will be the set of 
\begin_inset Formula $n$
\end_inset

network parameters.
 The probability density for this variable under the canonical distribution
 is defined by :
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
P(\boldsymbol{q})\propto\exp(-E(\bold q))\label{eq:HMC_desired}
\end{equation}

\end_inset

where 
\begin_inset Formula $E(\bold q)$
\end_inset

 is the 
\begin_inset Quotes eld
\end_inset

potential energy
\begin_inset Quotes erd
\end_inset

 function.
 Any probability density which is not zero at any point can be put in this
 form, by simply defining 
\begin_inset Formula $E(\bold q)=-\log P(\bold q)-\log Z$
\end_inset

, for any convenient 
\begin_inset Formula $Z$
\end_inset

.
\end_layout

\begin_layout Standard
To allow the use of Hamiltonian dynamics, we can introduce a 
\begin_inset Quotes eld
\end_inset

momentum
\begin_inset Quotes erd
\end_inset

 variable, 
\begin_inset Formula $\bold p$
\end_inset

.
 The canonical distribution over the 
\begin_inset Quotes eld
\end_inset

phase space
\begin_inset Quotes erd
\end_inset

 of 
\begin_inset Formula $\boldsymbol{q}$
\end_inset

 and 
\begin_inset Formula $\boldsymbol{p}$
\end_inset

together is then defined to be
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
P(\bold q,\boldsymbol{p})\propto\exp(-H(\bold q,\boldsymbol{p}))\label{eq:HMC_hamiltonian}
\end{equation}

\end_inset

where 
\begin_inset Formula $H(\bold q,\boldsymbol{p})=E(\bold q)+K(\bold p)$
\end_inset

 is the 
\begin_inset Quotes eld
\end_inset

Hamiltonian
\begin_inset Quotes erd
\end_inset

 function, which gives the total energy.
 
\begin_inset Formula $K(\bold p)$
\end_inset

 is the 
\begin_inset Quotes eld
\end_inset

kinetic energy
\begin_inset Quotes erd
\end_inset

 due to the momentum, for which the usual choice is :
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
K(\bold p)=\sum_{i=1}^{n}\frac{p_{i}^{2}}{2m_{i}}\label{eq:kinetic energy}
\end{equation}

\end_inset

The 
\begin_inset Formula $m_{i}$
\end_inset

 are the 
\begin_inset Quotes eld
\end_inset

masses
\begin_inset Quotes erd
\end_inset

 associated with each component.
 It is possible to adjust the masses for each component to improve efficiency,
 however for the rest of the report and project we take all of them to be
 one.
 
\end_layout

\begin_layout Standard
Since in the distribution of equation (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:HMC_hamiltonian"

\end_inset

), 
\begin_inset Formula $\boldsymbol{q}$
\end_inset

 and 
\begin_inset Formula $\boldsymbol{p}$
\end_inset

 are independent, the marginal distribution of 
\begin_inset Formula $\bold q$
\end_inset

 is the same as that of equation (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:HMC_desired"

\end_inset

), from which we intend to sample.
 We can proceed by defining a Markov chain that converges to the canonical
 distribution for 
\begin_inset Formula $\boldsymbol{q}$
\end_inset

 and 
\begin_inset Formula $\boldsymbol{p}$
\end_inset

.
 The values of 
\begin_inset Formula $\boldsymbol{p}$
\end_inset

 can then be simply ignored when estimating expectations of functions of
 
\begin_inset Formula $\boldsymbol{q}$
\end_inset

.
\end_layout

\begin_layout Standard
The overall dynamics of the system with fixed total energy can be expressed
 by the following equations:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align}
\frac{dq_{i}}{d\tau} & =+\frac{\partial H}{\partial p_{i}}=\frac{p_{i}}{q_{i}}\\
dq_{i} & =-\frac{\partial H}{\partial q_{i}}=-\frac{\partial E}{\partial q_{i}}
\end{align}

\end_inset

where 
\begin_inset Formula $\tau$
\end_inset

, is the fictitious time in which the state evolves.
 To be able to run these dynamics, we must be able to compute the partial
 derivatives of 
\begin_inset Formula $E$
\end_inset

 with respect to the the 
\begin_inset Formula $q_{i}$
\end_inset

.
 Radford Neal in his book
\begin_inset CommandInset citation
LatexCommand cite
key "neal1996bayesian"

\end_inset

 shows that the transformation preserves volume and is reversible.
 Therefore the transformation can be used as the transition operator for
 a Markov chain and will leave 
\begin_inset Formula $P(\boldsymbol{q})$
\end_inset

 invariant.
 Evolution under the Hamiltonian dynamics will not sample ergodically from
 
\begin_inset Formula $P(\boldsymbol{q,\boldsymbol{p}})$
\end_inset

 because the value of the total energy 
\begin_inset Formula $H$
\end_inset

 is constant.
 Regions with different values of 
\begin_inset Formula $H$
\end_inset

 are never visited.
 To avoid this, HMC alternates between performing deterministic dynamical
 transitions and stochastic Gibbs sampling updates of the momentum.
 Since 
\begin_inset Formula $\boldsymbol{q}$
\end_inset

 and 
\begin_inset Formula $\boldsymbol{p}$
\end_inset

 are independent , 
\begin_inset Formula $\boldsymbol{p}$
\end_inset

 may be updated without reference to 
\begin_inset Formula $\boldsymbol{q}$
\end_inset

.
 For the kinetic energy function of equation (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:kinetic energy"

\end_inset

), this is easily done, each of the 
\begin_inset Formula $p_{i}$
\end_inset

 have independent Gaussian distributions which are trivial to sample from.
 These updates of 
\begin_inset Formula $\boldsymbol{p}$
\end_inset

 change the total energy 
\begin_inset Formula $H$
\end_inset

 and hence allow the entire phase space to be explored.
 
\end_layout

\begin_layout Standard
The length in fictitious time of the trajectories is an adjustable parameter.
 It is generally better to use trajectories that result in large changes
 to 
\begin_inset Formula $\boldsymbol{q}$
\end_inset

 to avoid the random walk like effects that would result from randomizing
 the momentum after every short trajectory.
 
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Maybe include details rather than say refer to the book
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Maybe include figures about HMC 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Framework of Hamiltonian dynamics is exploited by casting the probabilistic
 simulation in the form of a Hamiltonian system.
 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
In practice, Hamiltonian dynamics can not be simulated exactly, but can
 only be approximated by some discretization using finite time steps.
 This will necessarily introduce numerical errors hence we need a scheme
 that minimizes the impact of these errors.
 In the 
\emph on
leapfrog 
\emph default
discretization, a single step finds the approximations to the position and
 momentum, 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
\lang english

\begin_inset Formula $(\boldsymbol{q^{*},\boldsymbol{p^{*}}})$
\end_inset


\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
\lang british
 at time 
\begin_inset Formula $\tau+\epsilon$
\end_inset

 from 
\begin_inset Formula $(\boldsymbol{q,\boldsymbol{p}})$
\end_inset

 at time 
\begin_inset Formula $\tau$
\end_inset

 as follows:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align}
p_{i}(\tau+\frac{\epsilon}{2}) & \;=\; p_{i}(\tau)\;-\;\frac{\epsilon}{2}\frac{\partial E}{\partial q_{i}}(\mathbf{q}(\tau))\label{eq:leapfrog}\\
q_{i}(\tau+\epsilon) & \;=\; q_{i}(\tau)\;+\;\epsilon\frac{p_{i}(\tau+\frac{\epsilon}{2})}{mi}\\
p_{i}(\tau+\epsilon) & \;=\; p_{i}(\tau+\frac{\epsilon}{2})\;-\;\frac{\epsilon}{2}\frac{\partial E}{\partial q_{i}}(\mathbf{q}(\tau+\epsilon))\label{eq:leapfrog end}
\end{align}

\end_inset

The leapfrog step consists of a half-step for the 
\begin_inset Formula $p_{i}$
\end_inset

 , followed by a full step for 
\begin_inset Formula $q_{i}$
\end_inset

, and another half-step for the 
\begin_inset Formula $p_{i}$
\end_inset

.
 To follow the dynamics for some period of time, 
\begin_inset Formula $\Delta\tau,$
\end_inset

 a value of 
\begin_inset Formula $\epsilon$
\end_inset

 is chosen which is small enough to give an acceptable error, and equations
 (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:leapfrog"

\end_inset

) - (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:leapfrog end"

\end_inset

) are applied for 
\begin_inset Formula $L=\Delta\tau/\epsilon$
\end_inset

 steps in order to reach the target time.
 The momentum variable is negated at this point to ensure that if we were
 to perform a dynamical transition starting at the end stage, we will get
 back to the initial stage.
\end_layout

\begin_layout Standard
In the 
\emph on
leapfrog
\emph default
 discretization scheme, the phase space volume remains preserved despite
 the discretization.
 The dynamics are also easily reversible.
 However, the value of 
\begin_inset Formula $H$
\end_inset

 no longer remains exactly constant, this can introduce bias in the simulation.
 
\end_layout

\begin_layout Standard
HMC cancels these effects exactly by adding a Metropolis accept/reject stage
 after 
\begin_inset Formula $L$
\end_inset

 leapfrog steps.
 If the original state is 
\begin_inset Formula $(\boldsymbol{q,\boldsymbol{p}})$
\end_inset

, and we get to the new state 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
\lang english

\begin_inset Formula $(\boldsymbol{q^{*},\boldsymbol{p^{*}}})$
\end_inset


\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
\lang british
 after 
\begin_inset Formula $L$
\end_inset

 leapfrog steps, the new state is then treated as a proposal state and is
 accepted with probability:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
min(1,\frac{\exp(-H(\boldsymbol{q^{*},\boldsymbol{p^{*}}}))}{\exp(-H(\boldsymbol{q,\boldsymbol{p}}))}
\end{equation}

\end_inset

In detail , given the number of of leapfrog steps, L, a dynamical transition
 is performed as follows:
\end_layout

\begin_layout Enumerate
Start from the current state, 
\begin_inset Formula $(\boldsymbol{q,\boldsymbol{p}})=(\boldsymbol{\hat{q}}(0),\boldsymbol{\hat{p}}(0))$
\end_inset

.
 Perform 
\begin_inset Formula $L$
\end_inset

 leapfrog steps with a step-size of 
\begin_inset Formula $\epsilon$
\end_inset

, resulting in the state 
\begin_inset Formula $(\boldsymbol{\hat{q}}(\epsilon L),\boldsymbol{\hat{p}}(\epsilon L))$
\end_inset


\end_layout

\begin_layout Enumerate
Negate the momentum variables, to produce the proposal state 
\begin_inset Formula $(\boldsymbol{q^{*},\boldsymbol{p}^{*}})=(\boldsymbol{\hat{q}}(\epsilon L),\boldsymbol{-\hat{p}}(\epsilon L))$
\end_inset

.
\end_layout

\begin_layout Enumerate
Accept 
\begin_inset Formula $(\boldsymbol{q^{*},\boldsymbol{p}^{*}})$
\end_inset

 as the new state with probability:
\begin_inset Formula 
\[
min(1,\frac{\exp(-H(\boldsymbol{q^{*},\boldsymbol{p^{*}}}))}{\exp(-H(\boldsymbol{q,\boldsymbol{p}}))}
\]

\end_inset

otherwise let the new state be the same as the old.
\end_layout

\begin_layout Standard
It is important to maintain satisfactory acceptance rates by tuning the
 step-sizes 
\begin_inset Formula $\epsilon$
\end_inset

 and the number of leapfrog steps 
\begin_inset Formula $L$
\end_inset

.
 A simple adaptive version of HMC is used in this report as implemented
 in the code accompanying 
\begin_inset CommandInset citation
LatexCommand cite
key "Ranzato10factored3-way"

\end_inset

.
 
\begin_inset Note Note
status open

\begin_layout Plain Layout
ask THang
\end_layout

\end_inset

We track the average acceptance rate of the HMC move proposals, using an
 exponential moving average.
 If the average acceptance larger than a target acceptance rate, we can
 increase the step-size ,
\begin_inset Formula $\epsilon$
\end_inset

, to improve the mixing rate of the Markov chain.
 If the acceptance rate is too low, the step-size can be decreased to improve
 the acceptance rate but yielding a more conservative mixing rate.
\end_layout

\begin_layout Subsection
Intriguing properties of Neural Networks 
\end_layout

\begin_layout Standard
Some interesting properties of neural networks have been recently investigated.
 
\end_layout

\begin_layout Section
Bayesian Optimisation
\begin_inset CommandInset label
LatexCommand label
name "sec:Bayesian-Optimisation"

\end_inset


\end_layout

\begin_layout Standard
Bayesian Optimisation is the model based optimisation of black box functions.
 
\end_layout

\begin_layout Subsection
High Dimensional Bayesian Optimisation 
\end_layout

\begin_layout Standard
An original goal of this project was to explore Bayesian Optimisation in
 high dimensions .
 This has not been thoroughly explored in the available literature.
 However two good papers have tried to tackle this problem.
 
\end_layout

\begin_layout Subsubsection
Rembo
\end_layout

\begin_layout Standard
Developed by Ziyu wang ..
 
\end_layout

\begin_layout Subsubsection
Add-GP-UCB
\end_layout

\begin_layout Standard
Was developed at CMU by ..
\end_layout

\begin_layout Section
Bayesian Optimisation with Bayesian Neural Networks
\begin_inset CommandInset label
LatexCommand label
name "sub:Using-a-full_BNN"

\end_inset


\end_layout

\begin_layout Subsection
Bayesian Neural Networks
\end_layout

\begin_layout Standard
Neural networks discussed in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Neural-Networks"

\end_inset

, used maximum likelihood to determine the network parameters.
 Regularized maximum likelihood can be interpreted as MAP (maximum a posteriori)
 approach in which the regularizer can be viewed as the logarithm of a prior
 parameter distribution.
 However, a full Bayesian treatment of neural networks, requires us to marginali
ze over the posterior distribution of parameters
\begin_inset CommandInset citation
LatexCommand cite
key "bishop2006pattern,neal1996bayesian"

\end_inset

.
 In this section we discuss an implementation of a Bayesian neural network
 in which network parameters are updated using the hybrid Monte Carlo algorithm.
 The hyperparameters are updated separately by using Gibbs sampling.
 
\end_layout

\begin_layout Standard
Bayesian learning for neural networks is a difficult problem, due to the
 typically complex nature of the posterior distribution.
 The hybrid Monte Carlo(HMC) algorithm is particularly suitable for this
 problem, due to its avoidance of random walk behaviour as discussed in
 section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sub:The-hybrid-Monte"

\end_inset

.
\end_layout

\begin_layout Standard
A neural network can be parametrized by its weights and biases, collectively
 denoted as 
\begin_inset Formula $\boldsymbol{\theta},$
\end_inset

 that define what network from input to output is denoted by the network.
 This function can then be written as 
\begin_inset Formula $f(\mathbf{x},\boldsymbol{\theta})$
\end_inset

, where x is the input to the network.
 A prior can be defined on the network parameters, this prior can depend
 on some hyperparameters, 
\begin_inset Formula $\mathbf{\gamma}$
\end_inset

.
 The prior density for the parameters can be written as 
\begin_inset Formula $P(\theta\,|\,\gamma)$
\end_inset

 and the prior density for the hyperparameters can be written as 
\begin_inset Formula $P(\gamma).$
\end_inset

 We have a training data-set given by 
\begin_inset Formula $\mathbf{D}=\{(x^{(1)},y^{(1)}),\dots,(x^{(n)},y^{(n)})\}$
\end_inset

 , consisting of independent pairs of input values, 
\begin_inset Formula $x^{(i)}$
\end_inset

, and target values, 
\begin_inset Formula $y^{(i)}$
\end_inset

.
 We aim to model the conditional distribution of target values given the
 input values.
 The required conditional probability density is given by 
\begin_inset Formula $P(y\,|\, x,\theta,\gamma).$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
add illustration of neural network from Neal pg17.
 Prior just show 10 random NN's .
 Then train using HMC then draw 10 randomNN's 
\end_layout

\end_inset


\begin_inset Note Note
status open

\begin_layout Plain Layout
tHEANO WAS used
\end_layout

\end_inset


\end_layout

\begin_layout Standard
The ultimate objective is to predict the target value for new test cases,
 
\begin_inset Formula $y^{(n+1)},$
\end_inset

 given the corresponding inputs, 
\begin_inset Formula $x^{(n+1)},$
\end_inset

using the information in the training set.
 To make the prediction we require the posterior distribution for 
\begin_inset Formula $\boldsymbol{\theta}$
\end_inset

 and 
\begin_inset Formula $\boldsymbol{\gamma}$
\end_inset

, this is proportional to the product of the prior and the likelihood due
 to the training cases:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
P(\boldsymbol{\theta,\boldsymbol{\gamma}\,|\,\mathbf{D}})\ \propto\ P(\gamma)P(\theta|\gamma)\prod_{c=1}^{n}P(y^{(c)}\,|\, x^{(c)},\boldsymbol{\theta},\boldsymbol{\gamma})\label{eq:probability NN}
\end{equation}

\end_inset

Predictions on new data-points can then be made by integration with respect
 to this posterior distribution.
 The prediction on new data-points is then given by:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
P(y^{(n+1)}|x^{(n+1)},\boldsymbol{D})\;=\;\int P(y^{(n+1)}|x^{(n+1)},\boldsymbol{\theta},\boldsymbol{\gamma})P(\boldsymbol{\theta,\boldsymbol{\gamma}\,|\,\mathbf{D}})\, d\mathbf{\theta}\, d\mathbf{\gamma}
\end{equation}

\end_inset

For a regression model, the prediction that minizes the expected squared-error
 loss is the mean of the predictive distribution.
 It is given by:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\hat{y}^{(n+1)}\;=\;\int f(x^{(n+1)},\boldsymbol{\theta})P(\boldsymbol{\theta,\boldsymbol{\gamma}\,|\,\mathbf{D}})\, d\mathbf{\theta}\, d\mathbf{\gamma}\label{eq:prediction}
\end{equation}

\end_inset

Since these integrals take the form of expectation of functions with respect
 to the posterior distribution, they can be tackled by using a Markov chain
 Monte Carlo approach.
 We can approximate the integral with the average value of the function
 over a sample of values from the posterior.
 
\end_layout

\begin_layout Standard
The hybrid Monte Carlo method discussed in section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sub:The-hybrid-Monte"

\end_inset

 is used to generate samples from the posterior.
 To apply this method, we first need to formulate the desired distribution
 in terms of a potential energy function.
 Since the objective is to sample from the posterior distribution for network
 parameters, the energy function will be a function of these parameters.
 The parameters 
\begin_inset Formula $\boldsymbol{\theta}$
\end_inset

, will play the role of the the 
\begin_inset Quotes eld
\end_inset

position
\begin_inset Quotes erd
\end_inset

 variables, 
\begin_inset Formula $\boldsymbol{q},$
\end_inset

 of an imaginary physical system.
 The hyperparameters remain fixed throughout one hybrid Monte Carlo update.
 Hence we can ignore all energy terms depending only on the hyperparameters.
 For the generic case described by equation (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:probability NN"

\end_inset

) , the potential energy can be calculated by taking the negative log of
 the prior and the likelihood due to the trainig cases, as follows:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
E(\boldsymbol{\theta})=F(\boldsymbol{\gamma})-\log P(\mathbf{\theta}|\mathbf{\gamma})-\sum_{c=1}^{n}P(y^{(c)}\,|\, x^{(c)},\boldsymbol{\theta},\boldsymbol{\gamma})
\end{equation}

\end_inset

where 
\begin_inset Formula $F(\boldsymbol{\gamma})$
\end_inset

 is any convenient function of the hyperparameters.
 The detailed form of the energy function will obviously depend on the network
 architecture, the prior, and the data model used.
 For the purpose of this project we put a Gaussian prior with mean zero
 and standard deviation 
\begin_inset Formula $\sigma_{p}$
\end_inset

 on each of the network parameters.
 We also assume that the targets are a single real value which can be modelled
 with a Gaussian noise distribution with standard deviation 
\begin_inset Formula $\sigma_{n}$
\end_inset

.
 The hyperparameters are then 
\begin_inset Formula $\boldsymbol{\gamma}=\{\tau_{p}\,,\,\tau_{n}\}$
\end_inset

, where 
\begin_inset Formula $\tau_{p}=\sigma_{p}^{-2}$
\end_inset

 and 
\begin_inset Formula $\tau_{n}=\sigma_{n}^{-2}$
\end_inset

.
 The variances are expressed in terms of the associated preision for convenience.
 The resulting potential energy function is :
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
E(\boldsymbol{\theta})\;=\;\tau_{p}\sum_{i=1}^{k}\frac{\theta_{i}^{2}}{2}+\tau\sum_{c=1}^{n}\frac{(y^{(c)}-f(x^{(c)},\theta))^{2}}{2}
\]

\end_inset

where k is the total number of parameters in the neural net i.e 
\begin_inset Formula $\boldsymbol{\theta}=\{\theta_{1},\dots,\theta_{k}\}$
\end_inset

, this includes both the weights and the biases associated with the network.
\end_layout

\begin_layout Standard
It can be noted that this energy function is similar to the error function
 which is minimized for training regularized conventional networks.
 However the objective here is not to minimise the total energy , but rather
 to sample from the canonical distribution represented by the energy.
\end_layout

\begin_layout Standard
We can proceed by introducing the momentum variable, 
\begin_inset Formula $\boldsymbol{p}$
\end_inset

.
 The 
\begin_inset Quotes eld
\end_inset

position
\begin_inset Quotes erd
\end_inset

 variable, 
\begin_inset Formula $\boldsymbol{q}$
\end_inset

 can be associated with the parameters 
\begin_inset Formula $\boldsymbol{\theta}$
\end_inset

 of the network.
 We then use the hybrid Monte Carlo algorithm to generate samples from the
 posterior of the parameters.
 These samples are then used to approximate the integral in equation (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:prediction"

\end_inset

) to make predictions on new data-points.
 
\end_layout

\begin_layout Standard
We plan to use the predictions from the neural network to perform Bayesian
 Optimisation using the GP-LCB acquisition function
\begin_inset CommandInset citation
LatexCommand cite
key "brochu2010tutorial"

\end_inset

, which is given by:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray}
LCB(x) & = & \mu(x)\text{âˆ’}K\sigma(x)
\end{eqnarray}

\end_inset

where 
\begin_inset Formula $\mu(x)$
\end_inset

 is the prediction at point 
\begin_inset Formula $x$
\end_inset

 as given by equation (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:muLCB-1"

\end_inset

) , and 
\begin_inset Formula $\sigma(x)$
\end_inset

 is the standard deviation at point 
\begin_inset Formula $x$
\end_inset

 calculated in equation (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:sigmaLCB"

\end_inset

).
 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray}
\mu(x) & = & \int f(x,\boldsymbol{\theta})P(\boldsymbol{\theta,\boldsymbol{\gamma}\,|\,\mathbf{D}})\, d\mathbf{\theta}\, d\mathbf{\gamma}\label{eq:muLCB-1}\\
\sigma(x) & = & \sqrt{\int(f(x,\boldsymbol{\theta})-\mu(x))^{2}P(\boldsymbol{\theta,\boldsymbol{\gamma}\,|\,\mathbf{D}})\, d\mathbf{\theta}\, d\mathbf{\gamma}}\label{eq:sigmaLCB}
\end{eqnarray}

\end_inset

With 
\begin_inset Formula $N$
\end_inset

 , samples from the posterior distribution of the parameters,
\begin_inset Formula $\{\boldsymbol{\theta_{1}},\dots,\boldsymbol{\theta_{N}}\}$
\end_inset

, we can approximate the required integrals by using MCMC methods to give:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray}
\mu(x) & = & \frac{1}{N}\sum_{i=1}^{N}f(x,\boldsymbol{\theta_{i}})\label{eq:muLCB-MCMC}\\
\sigma(x) & = & \sqrt{\frac{1}{N}\sum_{i=1}^{N}(f(x,\boldsymbol{\theta_{i}})-\mu(x))^{2}}\label{eq:sigmaLCB-MCMC}
\end{eqnarray}

\end_inset


\end_layout

\begin_layout Subsubsection
Verifying correctness
\end_layout

\begin_layout Standard
To verify the correctness of the above implementation, we show some preliminary
 results on a dimensional synthetic sinusoidal dataset.
 The dataset was generated by evaluating the function in equation (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:synthetic_sinsoidal"

\end_inset

) at 20 random points and adding some noise to the observations.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
f(x)=\sin(7x)+\cos(17x)\;\;\;\;\;\;\;\text{for}\ x\in[-1,1]\label{eq:synthetic_sinsoidal}
\end{equation}

\end_inset

A neural network with 3 hidden layers each of width 50 units was used.
 The 
\begin_inset Formula $tanh$
\end_inset

 non-linearity was used in the neural network.
 The HMC sampler was used to draw 5000 samples from the posterior distribution.
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:trace_parameters_BNN"

\end_inset

 shows the trace of three randomly chosen parameters from the neural network.
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:The-posterior-parameter"

\end_inset

 shows the histogram of the of one of the parameters (
\begin_inset Formula $\theta_{2501}$
\end_inset

) .
 It can be seen that the markov chains have not converged to a single mode
 and instead move between several modes.
 This can be attributed to the large number of modes that are likely to
 be present in the posterior.
 The algorithm is trying to explain 20 training data-points with about 5000
 parameters in the neural network.
 Multiple configurations of the parameters can explain the training data
 very well.
 To confirm this we plot the predictions made by the neural networks represented
 by individual HMC samples.
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:samples fit"

\end_inset

 shows the predictions of three well spaced samples from the Markov chain.
 As expected the predictions fit the training points very well, but show
 a significant amount of variation far away from them.
 The predictions of individual samples can then be averaged according to
 equations (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:muLCB-MCMC"

\end_inset

) - (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:sigmaLCB-MCMC"

\end_inset

) to find the predictions of the Bayesian neural network.
 The results are shown in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:The-overall-fit_prelim"

\end_inset

, with the credible interval taken to be two standard deviations on either
 side of the mean.
 It can be seen that the predictions follow the true function reasonably
 well.
 The prediction uncertainity is also modelled very well, it is low near
 the training data-points and is higher away from them.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
comment on how prediction relates to samples
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset space \hfill{}
\end_inset


\begin_inset Graphics
	filename images/trace.png
	lyxscale 20
	width 50col%

\end_inset


\begin_inset space \hfill{}
\end_inset


\begin_inset Graphics
	filename images/posteriorW251.png
	lyxscale 20
	width 50col%

\end_inset


\begin_inset space \hfill{}
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Box Frameless
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "50text%"
special "none"
height "1pt"
height_special "totalheight"
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Trace plots of three randomly chosen parameters of the neural network 
\begin_inset CommandInset label
LatexCommand label
name "fig:trace_parameters_BNN"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset space \quad{}
\end_inset


\begin_inset Box Frameless
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "50text%"
special "none"
height "1pt"
height_special "totalheight"
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
The histogram of the parameter 2501 of the neural network.
 
\begin_inset CommandInset label
LatexCommand label
name "fig:The-posterior-parameter"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset space \hfill{}
\end_inset


\end_layout

\end_inset

 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Graphics
	filename images/multiple_samples.png
	lyxscale 30
	width 70col%

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
centering
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
The fit of neural networks from four different samples from the posterior
 distribution of the network parameters
\begin_inset CommandInset label
LatexCommand label
name "fig:samples fit"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Graphics
	filename images/BNN_simple_fit.png
	lyxscale 20
	width 70col%

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
centering
\end_layout

\end_inset


\begin_inset Caption

\begin_layout Plain Layout
The overall fit of the neural network, averaged over a large number of samples
 from the neural network
\begin_inset CommandInset label
LatexCommand label
name "fig:The-overall-fit_prelim"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Maybe talk about GIbbs sampling of hypers, talk about splitting the parameters
 into groups which can be handled separately
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Bayesian Optimisation results
\end_layout

\begin_layout Standard
The Bayesian neural network model was then used to perform Bayesian optimisation.
 The LCB
\begin_inset Note Note
status open

\begin_layout Plain Layout
elaborate
\end_layout

\end_inset

 acquisition function was used.
 To illustrate the process of Bayesian Optimisation, we show some intermediate
 optimisation results on the one dimensional Forrester function
\begin_inset CommandInset citation
LatexCommand cite
key "forrester2008engineering"

\end_inset

.
 Its global minima is 
\begin_inset Formula $f(x^{*})\approx-6.02074$
\end_inset

 at 
\begin_inset Formula $x^{*}\approx0.7572$
\end_inset

.
 The function is given by:
\begin_inset Formula 
\begin{equation}
f(x)=(6x-2)^{2}\sin(12x-4)\text{\;\;\;\;\;\;\; for}\ x\in[-1,1]\label{eq:forrester}
\end{equation}

\end_inset

In Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Bayesian-Opt-forrester-fit"

\end_inset

, we show the Bayesian optimisation algorithm in action.
 We start with four random train points in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Number-of-Tra4"

\end_inset

, then a Bayesian neural network(BNN) model is fit to the data to decide
 the next proposal point based on the acquisition function.
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Number-of-Tra5"

\end_inset

 shows the model after querying the forrester function at the new proposal
 point (
\begin_inset Formula $x^{p}=0$
\end_inset

).
 Figures 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Number-of-TP8"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Number-of-TP14"

\end_inset

 show the model after a few more function evaluations.
 The model mirrors the true function reasonably well and finds the true
 global minima in a reasonable number of function evaluations.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename images/v2BNNforresterNtrain4.png
	lyxscale 20
	width 50col%

\end_inset


\begin_inset Caption

\begin_layout Plain Layout
Number of function evaluations = 4
\begin_inset CommandInset label
LatexCommand label
name "fig:Number-of-Tra4"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset space \thinspace{}
\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename images/v2BNNforresterNtrain5.png
	lyxscale 20
	width 50col%

\end_inset


\begin_inset Caption

\begin_layout Plain Layout
Number of function evaluations = 5
\begin_inset CommandInset label
LatexCommand label
name "fig:Number-of-Tra5"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename images/v2BNNforresterNtrain8.png
	lyxscale 20
	width 50col%

\end_inset


\begin_inset Caption

\begin_layout Plain Layout
Number of function evaluations = 8
\begin_inset CommandInset label
LatexCommand label
name "fig:Number-of-TP8"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset space \thinspace{}
\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename images/v2BNNforresterNtrain14.png
	lyxscale 20
	width 50col%

\end_inset


\begin_inset Caption

\begin_layout Plain Layout
Number of function evaluations = 14
\begin_inset CommandInset label
LatexCommand label
name "fig:Number-of-TP14"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Bayesian Optimisation on Forrester Function.
 The posterior mean is the averaged prediction from a large number of samples
 from the posterior of the Bayesian neural network.
 The standard deviation of the predcitions of these samples was used to
 find the 95% confidence interval(95% C.I).
 
\begin_inset CommandInset label
LatexCommand label
name "fig:Bayesian-Opt-forrester-fit"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Note Note
status open

\begin_layout Plain Layout
Explain better
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Graphics
	filename images/objectiveForresterBestVals.png
	lyxscale 20
	width 50col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Forrester Function
\begin_inset CommandInset citation
LatexCommand cite
key "forrester2008engineering"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset space \thinspace{}
\end_inset


\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Graphics
	filename images/rosenbrock_2DBestVals.png
	lyxscale 20
	width 50col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Rosenbrock(2D) Function
\begin_inset CommandInset citation
LatexCommand cite
key "picheny2013benchmark"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Graphics
	filename images/mccormickBestVals.png
	lyxscale 20
	width 50col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
McCormick Function
\begin_inset CommandInset citation
LatexCommand cite
key "adorio2005mvf"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset space \thinspace{}
\end_inset


\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Graphics
	filename images/sixhumpcamelBestVals.png
	lyxscale 20
	width 50col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Six Hump Camel Function
\begin_inset CommandInset citation
LatexCommand cite
key "molga2005test"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Results of Bayesian Optimisation on some standard test functions.
 Evaluations using a Bayesian network(blue) and a Gaussian Process(green)
 are shown.
 Error bars represent the standard deviation of 50 runs.
 In each sub-figure we are minimizing an objective function.
 The vertical axis represents the running minimum function value.
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Note Note
status open

\begin_layout Plain Layout
Put in actual minimum values
\end_layout

\end_inset


\end_layout

\begin_layout Section
Conclusions
\end_layout

\begin_layout Standard
The results are very promising 
\end_layout

\begin_layout Subsection
Potential applications/Future work
\end_layout

\begin_layout Standard
This system promises to provide better results for high-dimensional functions.
 Add Nilesh's paper to this 
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard

\lang english
\begin_inset CommandInset bibtex
LatexCommand bibtex
btprint "btPrintAll"
bibfiles "bibliography"
options "bibtotoc,plain"

\end_inset


\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
\start_of_appendix
Risk Assessment Retrospective
\end_layout

\begin_layout Standard
The risk assessment submitted at the start of the project identified frequent
 computer use as the main potential hazard.
 This assessment was accurate and no other hazards were encountered during
 the execution of the project.
 To avoid repetitive stress injury from excessive computer use, frequent
 breaks were taken by the author.
 Several exercises and best practices
\begin_inset CommandInset citation
LatexCommand cite
key "dugan2011healthy"

\end_inset

 were employed to reduce the risk further.
 If the project is to be carried out again, the risk assessment will remain
 the same.
\end_layout

\end_body
\end_document
