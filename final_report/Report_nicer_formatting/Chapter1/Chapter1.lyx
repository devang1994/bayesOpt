#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass PhDThesisLyX
\options a4paper,titlepage,12pt,numbered
\use_default_options false
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize 12
\spacing onehalf
\use_hyperref false
\papersize default
\use_geometry false
\use_amsmath 1
\use_esint 0
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle fancy
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Chapter
Introduction 
\end_layout

\begin_layout Standard

\lang british
Traditionally machine learning algorithms have focussed on using a set of
 expertly curated features of inputs to make predictions.
 However in various tasks, especially in computer vision and natural language
 processing, the design of these features can be very complex.
 In many situations it can be difficult to know what features should be
 extracted.
 Neural networks excel at such tasks as they can discover not only the mapping
 from the representation to the output but the representation itself.
 This approach is called 
\emph on
representation learning.
 
\emph default
Learned representations often result in much better performance than can
 be obtained with hand-designed representations.
 They also allow machine learning systems to rapidly adapt to new tasks,
 with minimal human intervention.
 A representation learning algorithm can discover a good set of features
 for a complex task in hours to months.
 Manually designing features for a complex task can require a great deal
 of human time and effort; it can possibly take decades for an entire community
 of researchers.
 
\end_layout

\begin_layout Standard

\lang british
Such representational learning capabilities have allowed neural networks
 to reach state of the art performance in several applications including
 computer vision and natural language processing.
 They have been used to get human-level performance in several video games
 
\begin_inset CommandInset citation
LatexCommand cite
key "mnih2015human"

\end_inset

 and have been used in systems to master the game of 
\begin_inset Quotes eld
\end_inset

Go
\begin_inset Quotes erd
\end_inset

 
\begin_inset CommandInset citation
LatexCommand cite
key "silver2016mastering"

\end_inset

.
 An 
\emph on
autoencoder
\emph default
 network is an excellent example of a representation learning algorithm
 
\begin_inset CommandInset citation
LatexCommand cite
key "hinton2006reducing"

\end_inset

.
 An autoencoder is the combination of an 
\emph on
encoder
\emph default
 function that converts the input data into a diï¬€erent representation, and
 a 
\emph on
decoder
\emph default
 function that converts the new representation back into the original format.
 Autoencoders are trained to preserve as much information as possible when
 an input is run through the encoder and then the decoder.
 By having a 
\emph on
bottleneck 
\emph default
hidden layer as illustrated in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:An-illustration-autoencoder"

\end_inset

, the algorithm can automatically learn a low dimensional code to efficiently
 represent the input data.
 
\end_layout

\begin_layout Standard

\lang british
Most neural network applications use maximum likelihood estimates for the
 parameters of the neural network, such estimates can be found by minimising
 an error function such as the sum of squared errors between the target
 and predictions, for real-valued outputs.
 The backpropogation algorithm can be used to calculate the gradients of
 the error with respect to each of the parameters, this can then be used
 in a gradient based optimisation schedule such as Adam 
\begin_inset CommandInset citation
LatexCommand cite
key "kingma2014adam"

\end_inset

.
 However, for various applications, it is often desirable to estimate the
 posterior distribution of the parameters of the neural networks.
 We can use a Bayesian approach to neural network learning to find the posterior
 distribution of the network parameters by combining a simple prior on the
 network weights with the likelihoods of points observed in the training
 data 
\begin_inset CommandInset citation
LatexCommand cite
key "neal1996bayesian"

\end_inset

.
 Such a posterior distribution can give the predictive distributions of
 the target values in new 
\begin_inset Quotes eld
\end_inset

test
\begin_inset Quotes erd
\end_inset

 cases, given the input for that case, and the inputs and targets in the
 training cases.
 Having a predictive distribution on target values is particularly useful
 for sequential decision making and active learning tasks where it is important
 to know both, the prediction and the uncertainty associated with it.
 
\end_layout

\begin_layout Standard

\lang british
Bayesian optimisation is an active learning method, in which we try to optimise
 a black-box function 
\begin_inset Formula $f$
\end_inset

 which is expensive to evaluate.
 It has been successfully used in many wide ranging applications such as
 hyper-parameter tuning in expensive machine learning algorithms 
\begin_inset CommandInset citation
LatexCommand cite
key "snoek2012practical"

\end_inset

, experiment design 
\begin_inset CommandInset citation
LatexCommand cite
key "jones1998efficient"

\end_inset

, optimizing control strategies in complex systems 
\begin_inset CommandInset citation
LatexCommand cite
key "cully2015robots"

\end_inset

, and scientific simulation based studies 
\begin_inset CommandInset citation
LatexCommand cite
key "brochu2010tutorial"

\end_inset

.
 Bayesian optimization finds the optimum of 
\emph on

\begin_inset Formula $f$
\end_inset


\emph default
 by using the information from all the previous evaluations of the function
 to learn a distribution over functions.
 The algorithm then endeavours to optimise the function in as few queries
 as possible by managing exploration and exploitation 
\begin_inset CommandInset citation
LatexCommand cite
key "mockus1978application"

\end_inset

.
 An accurate and cheap model for the distribution over functions is critical
 to the effectiveness of the approach, typically Gaussian processes (GPs)
 
\begin_inset CommandInset citation
LatexCommand cite
key "rasmussen2006gaussian"

\end_inset

 are used to model this distribution over functions.
 However, GPs can only model a limited class of functions well.
 The class of functions that can be modelled by the GP depends on its covariance
 function which encodes the assumptions about the function.
 A more expressive covariance function can increase the expressivity of
 the GP, however the predictive distribution at each point is still restricted
 to be a Gaussian.
 Moreover, choosing the structural form of the covariance function is a
 very challenging task and requires significant human intervention 
\begin_inset CommandInset citation
LatexCommand cite
key "duvenaud2014automatic"

\end_inset

.
 Recently some work has been done on frameworks for automatic kernel structure
 discovery 
\begin_inset CommandInset citation
LatexCommand cite
key "duvenaud2013structure"

\end_inset

, however that framework also has limitations.
 This problem is exacerbated when using GPs on a high dimensional input
 space, which restricts the dimensionality of the functions which can be
 optimized by this method.
 Another problem with GPs is that they scale cubically with the number of
 observations, as they require the inversion of a dense covariance matrix.
 This can make it challenging to use them on functions which require a moderatel
y large number of evaluations to optimise.
 High dimensional functions and functions defined on a very large domain
 can often give rise to situations where a large number of function evaluations
 are needed for optimisation.
 
\end_layout

\begin_layout Standard

\lang british
In this project we aim to use the predictive distribution described by a
 Bayesian neural network (BNN) to model the distribution over functions,
 to perform Bayesian optimisation.
 This provides a very flexible prior on the functions and can model a much
 wider class of functions.
 This method also scales linearly with the number of observations and can
 hence allow optimisation when a large number of function evaluations are
 needed.
 Neural networks can employ multiple non-linear transforms to extract relevant
 information from high dimensional inputs.
 This can allow them to learn an accurate model in a high-dimensional input
 space to effectively perform Bayesian optimisation.
 Another possible extension of this approach to performing high dimensional
 Bayesian optimisation could be to use autoencoders to learn a mapping into
 a low-dimensional space in which Bayesian optimisation can be performed
 more easily.
 
\end_layout

\begin_layout Standard

\lang british
A prior is defined on the distribution of functions by defining simple priors
 on the parameters of the neural networks, this provides a very flexible
 overall prior on the distribution of functions.
 Generally Gaussian priors are used on the parameters of the network, it
 is known that in the limit of the number of hidden units going to infinity,
 the priors over the functions implied by the priors converges to a Gaussian
 process 
\begin_inset CommandInset citation
LatexCommand cite
key "neal1996bayesian"

\end_inset

.
 It is also possible to use other other stable priors to encode prior informatio
n about the functions.
 This provides us the opportunity to model non-smooth functions and other
 functions whose behaviours not easily described by the GPs.
 For instance, Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Functions-Cauchy-prior"

\end_inset

 shows two functions drawn from priors for a network where weights and biases
 for the output have Cauchy distributions(stable distribution with 
\begin_inset Formula $\alpha=1$
\end_inset

).
 The weights and biases into the hidden units have independent Gaussian
 distributions 
\begin_inset CommandInset citation
LatexCommand cite
key "neal1996bayesian"

\end_inset

.
 
\end_layout

\begin_layout Standard

\lang british
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout

\lang british
\begin_inset Graphics
	filename ../../images/CauchyPrior.png
	lyxscale 40
	width 80col%

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
centering
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\lang british
\begin_inset Caption

\begin_layout Plain Layout

\lang british
Functions drawn from Cauchy priors on for networks with step-function hidden
 units 
\begin_inset CommandInset citation
LatexCommand cite
key "neal1996bayesian"

\end_inset

.
 Function on the left are from a network with 150 hidden units, those on
 the right from a network with 10000 hidden units.
 
\begin_inset CommandInset label
LatexCommand label
name "fig:Functions-Cauchy-prior"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard

\lang british
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout

\lang british
\begin_inset Graphics
	filename ../../images/Autoencoder_structure.png
	width 80col%

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
centering
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\lang british
\begin_inset Caption

\begin_layout Plain Layout

\lang british
An illustration of an autoencoder network with a bottleneck hidden layer.
 Such a network can be used to automatically learn a low dimensional code
 for the input data 
\begin_inset CommandInset citation
LatexCommand cite
key "hinton2006reducing"

\end_inset

.
 
\begin_inset CommandInset label
LatexCommand label
name "fig:An-illustration-autoencoder"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard

\lang british
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout

\lang british
The Introduction (1) describes the Bayesian optimisation method and explains
 why Bayesian neural networks are well suited for performing Bayesian optimisati
on
\end_layout

\end_inset

The report is organised into 6 sections.
 In section (2) we give a brief introduction to neural networks and how
 they can be used to create probabilistic models.
 In section (3) we review Markov chain Monte Carlo methods including the
 Metropolis algorithm and the hybrid Monte Carlo algorithm.
 Section (4) serves as a brief introduction to Bayesian optimisation, we
 also briefly review the existing literature on performing Bayesian optimisation
 in high dimensions.
 In Section (5) we review Bayesian neural networks and describe how to use
 them for Bayesian optimisation, we compare the performance of our method
 to one using Gaussian processes and report competitive results.
 The Conclusion (6) presents the main conclusions of the project and suggests
 some ideas for future work.
\end_layout

\end_body
\end_document
