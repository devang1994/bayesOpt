#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass PhDThesisLyX
\options a4paper,titlepage,12pt,numbered
\use_default_options false
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize 12
\spacing onehalf
\use_hyperref false
\papersize default
\use_geometry false
\use_amsmath 1
\use_esint 0
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle fancy
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Chapter
Neural Networks
\end_layout

\begin_layout Standard

\lang british
Neural networks use multiple non-linear transforms to map from an input
 to an output.
 They are known to have excellent feature selection properties and define
 the current state of the art in object recognition and natural language
 processing tasks 
\begin_inset CommandInset citation
LatexCommand cite
key "Bengio-et-al-2015-Book"

\end_inset

.
 For this project we primarily deal with 
\begin_inset Quotes eld
\end_inset


\emph on
feedforward
\emph default

\begin_inset Quotes erd
\end_inset

 networks, these networks take in a set of real inputs, 
\begin_inset Formula $x_{i}$
\end_inset

, and then compute one or more output values, 
\begin_inset Formula $f_{k}(\boldsymbol{x})$
\end_inset

, using some number of layers of 
\emph on
hidden
\emph default
 units.
 Here, we illustrate the basic concepts of neural networks by considering
 a simple network with one hidden layer such as the one shown in Figure
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:An-illustration-NN"

\end_inset


\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout

\lang british
\begin_inset Graphics
	filename ../../images/neuralNetIllustration.png
	width 70col%

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
centering
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\lang british
\begin_inset Caption

\begin_layout Plain Layout

\lang british
An illustration of a neural network with one hidden layer.
 The input units at the bottom are fixed to their value depending on the
 data point.
 The values of the hidden units are then computed, followed by the values
 of the output units.
 The value for a hidden or output unit is a function of the weighted sum
 of values it receives from the units that are connected to it via the arrows.
 More hidden layers can be trivially added between the existing hidden units
 and the output units 
\begin_inset CommandInset citation
LatexCommand cite
key "neal1996bayesian"

\end_inset

.
\begin_inset CommandInset label
LatexCommand label
name "fig:An-illustration-NN"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset

.
 The outputs can be calculated as follows:
\end_layout

\begin_layout Standard

\lang british
\begin_inset Formula 
\begin{eqnarray}
f_{k}(\boldsymbol{x}) & = & b_{k}+\sum_{j}v_{jk}h_{j}(x)\\
h_{j}(\boldsymbol{x)} & = & \tanh(a_{j}+\sum_{i}u_{ij}x_{i})
\end{eqnarray}

\end_inset

Here, 
\begin_inset Formula $u_{ij}$
\end_inset

 is the 
\emph on
weight 
\emph default
on the connection from the input unit 
\emph on

\begin_inset Formula $i$
\end_inset

 
\emph default
to the hidden unit 
\begin_inset Formula $j$
\end_inset

; similarly, 
\begin_inset Formula $v_{jk}$
\end_inset

 , is the weight on the connection from the hidden unit 
\begin_inset Formula $j$
\end_inset

 to the output unit 
\begin_inset Formula $k$
\end_inset

.
 The 
\begin_inset Formula $a_{j}$
\end_inset

 and 
\begin_inset Formula $b_{k}$
\end_inset

 are the 
\emph on
biases
\emph default
 of the hidden units and the output units respectively.
 These weights and networks collectively constitute the set of parameters
 of the network.
 
\end_layout

\begin_layout Standard

\lang british
Each output value, 
\begin_inset Formula $f_{k}(\boldsymbol{x})$
\end_inset

, is a weighted sum of the last hidden unit values, plus a bias.
 Each hidden unit computes a similar weighted sum of input values, and then
 passes it though a non-linear 
\emph on
activation function.
 
\emph default
In this project we choose the hyperbolic tangent (tanh) as the activation
 function, it is an asymmetric function of sigmoidal shape as shown in Figure
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:The-Hyperbolic-Tangent"

\end_inset

.
 Its value is close to 
\begin_inset Formula $-1$
\end_inset

 for large negative numbers,
\begin_inset Formula $+1$
\end_inset

 for large positive numbers and zero for zero argument.
 
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout

\lang british
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\lang british
\begin_inset Graphics
	filename ../../images/relu.png
	lyxscale 30
	width 50col%

\end_inset


\end_layout

\begin_layout Plain Layout

\lang british
\begin_inset Caption

\begin_layout Plain Layout

\lang british
The Rectified Linear Activation function
\begin_inset CommandInset label
LatexCommand label
name "fig:The-Rectified-LinearU"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\lang british
\begin_inset Graphics
	filename ../../images/tanh.png
	lyxscale 30
	width 50col%

\end_inset


\end_layout

\begin_layout Plain Layout

\lang british
\begin_inset Caption

\begin_layout Plain Layout

\lang british
The Hyperbolic Tangent Activation function
\begin_inset CommandInset label
LatexCommand label
name "fig:The-Hyperbolic-Tangent"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\lang british
\begin_inset Caption

\begin_layout Plain Layout

\lang british
Here we show the rectified linear(ReLU) and the hyperbolic tangent(tanh)
 activation function.
 The ReLU function is very popular with deep feedforward networks.
 Being very close to linear they preserve many properties that make linear
 models easy to optimize with gradient based methods.
 However, they are not smooth, the abrupt changes in gradients associated
 with them can lead to issues when building probabilistic models 
\begin_inset CommandInset citation
LatexCommand cite
key "Snoek:2015aa"

\end_inset

.
 However, tanh functions are smooth over their entire domain and are hence
 suited for building probabilistic models.
\end_layout

\end_inset


\end_layout

\end_inset

 The Rectified linear unit (ReLU) function is also a very popular activation
 function, being very close to linear, they preserve many properties that
 make linear models easy to optimise with gradient based methods.
 They are particularly popular for training 
\begin_inset Quotes eld
\end_inset

deep
\begin_inset Quotes erd
\end_inset

 neural network models, where their trainability allows the network to achieve
 an excellent performance.
 However their un-smooth nature, with abrupt changes in their gradient can
 cause issues when using them to build probabilistic models.
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:The-Rectified-LinearU"

\end_inset

 shows the ReLU activation function.
 Use of a non-linear activation function allows the overall function defined
 by the neural network to be non-linear.
 It allows the hidden units to potentially represent 
\begin_inset Quotes eld
\end_inset

hidden features
\begin_inset Quotes erd
\end_inset

 in the input that can be useful in computing the appropriate outputs.
 
\end_layout

\begin_layout Standard

\lang british
It has been shown that a neural network with one hidden layer can approximate
 any function defined on a compact domain arbitrarily closely, if enough
 hidden units are used 
\begin_inset CommandInset citation
LatexCommand cite
key "Hornik1991251,cybenko1989approximation,funahashi1989approximate"

\end_inset

.
 Nevertheless, more elaborate network architectures have advantages and
 are often used.
 Multiple hidden layers can be easily stacked between the current hidden
 units and the output units.
 Such 
\begin_inset Quotes eld
\end_inset

deep
\begin_inset Quotes erd
\end_inset

 networks are commonly used for a wide variety of machine learning applications.
 However it should be noted that in feedforward networks, connections are
 not allowed to form cycles.
 This is important to allow the value of the output to be computed in a
 single forward pass, in time proportional to the number of network parameters.
 
\end_layout

\begin_layout Subsection

\lang british
Training neural networks
\end_layout

\begin_layout Standard

\lang british
We can define probabilistic models for regression tasks by using the network
 outputs to define the conditional distribution for the targets, 
\begin_inset Formula $\boldsymbol{y}$
\end_inset

, for various values of the input vector 
\begin_inset Formula $\boldsymbol{x}$
\end_inset

.
 For a regression model with real-valued targets, 
\begin_inset Formula $y_{k}$
\end_inset

, the conditional distribution of the targets might be defined to be Gaussian,
 with 
\begin_inset Formula $y_{k}$
\end_inset

 having a mean 
\begin_inset Formula $f_{k}(\boldsymbol{x})$
\end_inset

 and a standard deviation of 
\begin_inset Formula $\sigma_{k}$
\end_inset

.
 We often assume the different outputs to be independent, given the input.
 This gives:
\end_layout

\begin_layout Standard

\lang british
\begin_inset Formula 
\begin{eqnarray}
P(\boldsymbol{y}|\boldsymbol{x}) & = & \prod_{k}\frac{1}{\sqrt{2\pi\sigma_{k}^{2}}}\exp(-\frac{(f_{k}(\boldsymbol{x})-y_{k})^{2}}{2\sigma^{2}})\label{eq:gaussian noise model}
\end{eqnarray}

\end_inset

The 
\begin_inset Quotes eld
\end_inset

noise level
\begin_inset Quotes erd
\end_inset

, 
\begin_inset Formula $\sigma_{k}$
\end_inset

, might be fixed or treated as a hyper-parameter.
 The weights and biases of the neural network can be learnt on a set of
 
\emph on
training cases, 
\emph default

\begin_inset Formula $\mathbf{D}=\{(x^{(1)},y^{(1)}),\dots,(x^{(n)},y^{(n)})\}$
\end_inset

, giving independent examples of inputs, 
\begin_inset Formula $\boldsymbol{x}^{(i)}$
\end_inset

, and associated targets 
\begin_inset Formula $\boldsymbol{y}^{(i)}$
\end_inset

.
 Standard neural network training procedures adjust the weights and biases
 in the network so as to minimize a measure of 
\begin_inset Quotes eld
\end_inset

error
\begin_inset Quotes erd
\end_inset

 on the training cases, most commonly, the sum of the squared differences
 between the network outputs and targets.
 Minimization of this error measure is equivalent to maximum likelihood
 estimation of the Gaussian noise model of equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:gaussian noise model"

\end_inset

, since negative log likelihood with this model is proportional to the sum
 of squared errors.
 
\end_layout

\begin_layout Standard

\lang british
A gradient-based approach is commonly used to find the weights and biases
 that minimize the chosen error function.
 The derivatives of the error with respect to the weights and biases can
 be calculated by using the 
\emph on
backpropagation
\emph default
 algorithm 
\begin_inset CommandInset citation
LatexCommand cite
key "Bengio-et-al-2015-Book,rumelhart1988learning"

\end_inset

.
 Typically many local minima are present, but good solutions are often found
 despite this.
 
\end_layout

\begin_layout Standard

\lang british
Neural networks have a large number of parameters which makes overfitting
 a significant problem when training neural networks.
 To reduce overfitting a penalty term proportional to the sum of the squares
 of the weights and biases is often added to the error function.
 This modification is known as 
\emph on
weight decay, 
\emph default
since its effect is to bias the training procedure in favour of small weights.
 Determining the proper magnitude of weight decay is often difficult --
 too little weight decay, the network may 
\begin_inset Quotes eld
\end_inset

overfit
\begin_inset Quotes erd
\end_inset

, but too much weight decay, the network will 
\begin_inset Quotes eld
\end_inset

underfit
\begin_inset Quotes erd
\end_inset

 and ignore the data.
 Adding weight decay can be interpreted as having a Gaussian prior with
 mean zero and some standard deviation, 
\begin_inset Formula $\sigma_{w}$
\end_inset

, on the weights and biases of the neural network.
 
\end_layout

\begin_layout Standard

\lang british
Cross validation is often used to find an appropriate weight penalty 
\begin_inset CommandInset citation
LatexCommand cite
key "stone1974cross"

\end_inset

.
 In its simplest form, the amount of weight decay is chosen to optimise
 performance on a validation set separate from the cases used to estimate
 the network parameters.
 
\end_layout

\begin_layout Standard

\lang british
In the Bayesian approach to neural network learning, the objective is to
 find the predictive distribution for the target values in new 
\begin_inset Quotes eld
\end_inset

test
\begin_inset Quotes erd
\end_inset

 cases, given the input for that case, and the inputs and targets in the
 training cases (
\begin_inset Formula $\boldsymbol{D}$
\end_inset

).
 The predictive distribution is given by:
\end_layout

\begin_layout Standard

\lang british
\begin_inset Formula 
\begin{align}
P(y^{(n+1)}|\xvec^{(n+1)},\boldsymbol{D}) & =\int P(y^{(n+1)}|\xvec^{(n+1)},\boldsymbol{\theta})P(\boldsymbol{\theta|\mathbf{D}})d\thevec
\end{align}

\end_inset

Here, 
\begin_inset Formula $\thevec$
\end_inset

 represents the network parameters (weights and biases).
 The posterior density for these parameters is proportional to the product
 of the prior on them and the likelihood function given by:
\end_layout

\begin_layout Standard

\lang british
\begin_inset Formula 
\begin{align}
L(\boldsymbol{\theta|\mathbf{D}}) & =\prod_{c=1}^{n}P(y^{(c)}|\xvec^{(c)},\boldsymbol{\theta})
\end{align}

\end_inset

The distribution of the target values, 
\begin_inset Formula $y^{(i)}$
\end_inset

, given the corresponding inputs, 
\begin_inset Formula $\boldsymbol{x}^{(i)}$
\end_inset

, and the parameters of the network is defined by the type of model with
 which the network is being used.
 For the regression model, it is given by equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:gaussian noise model"

\end_inset

.
\end_layout

\end_body
\end_document
